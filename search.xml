<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hexo基本操作</title>
      <link href="/hexo-ji-ben-cao-zuo/"/>
      <url>/hexo-ji-ben-cao-zuo/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="写文章、发布文章"><a href="#写文章、发布文章" class="headerlink" title="写文章、发布文章"></a>写文章、发布文章</h2><p>首先在博客根目录下右键打开git bash，安装一个扩展<code>npm i hexo-deployer-git</code>。</p><p>然后输入<code>hexo new post "article title"</code>，新建一篇文章。</p><p>然后打开<code>D:\study\program\blog\source\_posts</code>的目录，可以发现下面多了一个文件夹和一个<code>.md</code>文件，一个用来存放你的图片等数据，另一个就是你的文章文件啦。</p><p>编写完markdown文件后，根目录下输入<code>hexo g</code>生成静态网页，然后输入<code>hexo s</code>可以本地预览效果，最后输入<code>hexo d</code>上传到github上。这时打开你的github.io主页就能看到发布的文章啦。</p><h2 id="备份博客源文件"><a href="#备份博客源文件" class="headerlink" title="备份博客源文件"></a>备份博客源文件</h2><p>有时候我们想换一台电脑继续写博客，这时候就可以将博客目录下的所有源文件都上传到github上面。</p><p>首先在github博客仓库下新建一个分支<code>hexo</code>，然后<code>git clone</code>到本地，把<code>.git</code>文件夹拿出来，放在博客根目录下。</p><p>然后<code>git branch -b hexo</code>切换到<code>hexo</code>分支，然后<code>git add .</code>，然后<code>git commit -m "xxx"</code>，最后<code>git push origin hexo</code>提交就行了。</p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning</title>
      <link href="/reinforcement-learning/"/>
      <url>/reinforcement-learning/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><center><h1>分层强化学习在空对空作战领域的研究</h1></center><blockquote><p>摘要——人工智能（AI）正在成为国防工业的一个重要组成部分，，DARPA最近的Alpha狗斗实验（ADT）证明了这一点。ADT寻求验证AI算法在模拟空战中驾驶F-16的可行性。作为ADT的参与者，洛克希德·马丁公司（LM）的方法将分层架构和最大熵强化学习相结合，通过奖励塑造整合专家知识，并支持策略模块化。这种方法在ADT决赛中（共计8位选手中）获得了第二名，并击败了美国空军（USAF）F-16武器教练课程的毕业生。</p><p>索引——分层强化学习，空战，飞行模拟</p></blockquote><center><h2>1、介绍</h2></center><p>美国国防部高级研究计划局（DARPA）成立的空战进化（ACE）——[[Demo1#title|demo1]]计划旨在自主空战方面进行推进和建立信任。在部署中，自主空战目前仅限于基于规则的系统，例如自动驾驶和避障。在战斗机飞行员群体中，学习视距内格斗（狗斗）包括了很多基础的飞行动作（BFM），而这些动作是成为一个可信任的协同作战飞行员的必备技能。为了使自动系统在更复杂的作战中有效，例如压制敌军防空，护送和定点保护，需要首先掌握BFMs。因此，ACE选择了狗斗作为在先进自动系统建立信任的起点。ACE的最终计划是在真实大小的飞机上进行实飞演习。</p><p>Alpha狗斗实验（ADT）是ACE计划的前身，，旨在降低风险。对于ADT，选择了八个队伍，方法从基于规则的系统到完全端到端的机器学习架构不等。通过实验，这些团队在高保真F-16飞行动力学模型中进行了了1v1的模拟狗斗测试。举办一系列对抗赛对抗各种敌对智能体：DARPA提供了执行不同行为动作的智能体（如快速水平飞行，模拟导弹拦截任务），其他竞争团队智能体和一位经验丰富的人类战斗机飞行员。</p><p>在本文中，我们将会介绍环境，我们的智能体设计，讨论对抗的结果，并概述我们计划进一步开发该技术的未来工作。我们的方法使用了分层强化学习并利用一些列根据当前参与环境动态选择的特定的策略。</p><center><h2>2、相关工作</h2></center><p>自20世纪50年代以来，人们一直在研究如何构建能够自主执行空战的算法。一些人已经使用基于规则的方法来解决这个问题，使用专家知识来制定在不同状态环境下使用的对抗策略。其他探索以各种方式将空对空场景编码成一个优化问题，通过计算解决。一些研究依赖于博弈论方法，在一组离散的动作上建立效用函数，而其他方法采用不同风格的动态规划（DP）算法。在许多这类论文中，为了在合理的时间内获得近似的最优解，在环境和算法复杂度之间做了权衡。一项著名的研究使用了遗传模糊树开发了一种能在AFSIM环境中击败美国空军武器学校毕业上校的智能体。</p><p>最近，深度强化学习已经被应用在该问题领域。例如，在一个定制的三维环境中训练一个智能体，该环境从15个离散动作中选择，并且能够击败一名人类。在AFSIM环境中评估各种学习算法和方案。一般来说，很多深度强化学习方法的研究要么利用低保真度/维度进行环境模拟，要么将动作空间抽象成高层行为或战术。</p><p>ADT模拟环境与许多其他工作相比具有独特的高保真性。该环境提供了一个F-16战机的飞行动力学模型，该模型具有六个自由度，并且支持对飞行控制系统的直接输入。该模型在开源的JSBSim软件中运行，这个软件通常被认为是非常精确的空气动力学建模软件。在这项工作中，我们概述了一个强化学习智能体的设计，并且该智能在ADT环境中体展现出更好的策略。</p><center><h2>3、背景</h2></center><h3 id="A-强化学习"><a href="#A-强化学习" class="headerlink" title="A. 强化学习"></a>A. 强化学习</h3><p>广泛的说，一个智能体可以被定义成“任何东西能够被看作感知其周围环境通过传感器并根据环境付诸行动”。一个强化学习智能体关心的是去做什么和采取什么行动去最大化一个熟知的奖励信号。奖励信号被环境提供，整个学习过程以重复迭代的试错方式进行。因此，强化学习不同于监督和无监督学习。监督学习是根据外部监督者提供的带有标签的训练集进行学习。无监督学习并不依靠标签，取而代之的是找寻一些数据中的隐藏结构。强化学习在学习样本被标识的情况下与监督学习类似，然而，在智能体学习过程中，这些流程动态执行通过记录与仿真环境交互。</p><p>在强化学习中，一个主要的挑战是在探索和开发之间进行权衡。当强化学习智能体通过执行动作与环境进行交互，它开始学习最终能够获得高奖励回报的选择。自然地，为了最大化接收的奖励，智能体应该开发已经通过选择高奖励行为学习到的策略。然而，为了发现最优动作，智能体不得不承担风险，探索新动作序列，这可能导致比当前最优动作价值更高的奖励。最受欢迎的方法是$\epsilon$贪心策略，这个策略中智能体通过概率$0&lt;\epsilon&lt;1$随机选择一个动作，或者以概率$1-\epsilon$贪心的选择最高价值的动作。直观地，智能体应该在开始探索更多，并且，随着训练过程的推进，智能体应该更多地开发。然而，知道最好的方式去探索是十分困难的，依赖环境的，并且仍然是一个研究中的活跃领域。另一个有趣且富有挑战的方面关于强化学习领域的是动作可能不仅仅影响即时奖励，也会影响到后续的奖励。因此，一个强化学习智能体必须学习权衡即时奖励和延迟奖励。</p><h3 id="B-马尔科夫决策过程"><a href="#B-马尔科夫决策过程" class="headerlink" title="B.马尔科夫决策过程"></a>B.马尔科夫决策过程</h3><p>马尔科夫决策过程（MDP）提供了数学形式去塑造序列决策问题。一个马尔科夫决策过程由状态集S，动作集A，一个转移函数T和一个奖励函数R组成，构成了$&lt;S,A,T,R&gt;$四元组。给定任何状态$s\in S$，选择一个动作$a\in A$将导致环境有状态转移概率$T(s,a,s’)\in[0,1]$转换到新状态$s’\in S$并且返还一个奖励$R(s,a)$。一个随机策略$\pi:S\rightarrow A$是一个从状态到每一个可能选择动作概率的映射，在这里$\pi(a|s)$代表给定状态s下选择动作a的概率。目标是找到最优策略$\pi*$，它提供了最高的奖励总和的期望：</p><p>$$\pi*=\arg \underset{\pi} {max} \mathbb{E}_{\pi} \{\sum_t^H\gamma^tr_{t+1}|s_0=s\}$$<br>$$lim_{1\to+\infty}P(|\frac{1}{n}\sum_i^nX_i-\mu|&lt;\epsilon)=1, i=1,…,n$$  </p><p>其中，$\gamma\in[0,1]$是折扣因子，$H$是时间范围，$r_t$是$R(s=s_t,a=a_t)$的简写。价值函数被定义成在状态s下遵循策略$\pi$的未来期望奖励，$V_{\pi}(s)=\mathbb{E}_ {\pi}\{\sum_t^H\gamma^tr_ {t+1}|s_s0=s\}$。小的$\gamma$值导致智能体关注短期奖励，大的值关注长期奖励。范围H代表马尔科夫巨册过程中的步数，当$H = +\infty$时表明是一个无穷时间的问题。如果MDP在一个特定的目标上终止了，乘坐回合任务，最后的状态称作终止状态，并且$\gamma$的值接近于1是推荐的。</p><p>一个重要的关于价值函数的概念是动作价值函数，定义如下：</p><p>$$ Q_ {\pi}(s,a)=\mathbb{E}_ \pi\{\sum_ t^H\gamma^tr_ {t+1}|s_0=s,a_0=a\}$$</p><p>估计Q函数被称作无模型方法，因为它不需要知道转移函数去计算最优策略，这是很多强化学习算法中的关键部分。</p><p>最优策略和相关的最优价值函数能够被估计通过使用动态规划（DP），条件是完整的环境模型，即奖励和转移概率是可利用的。否则，对于有限时间的集合任务，蒙特卡洛（MC）方法能够被用来估计期望奖励，然而这需要模拟整个集便于估计价值函数并更新策略，这可能需要大量的样本去拟合。在强化学习中，MDP传统使用时间微分算法（TD）解决，这种方法价值函数一步步被估计，就像DP算法一样，直接源于原始数据，像MC算法一样，不需要转移函数。</p><h3 id="C-最大熵强化学习"><a href="#C-最大熵强化学习" class="headerlink" title="C.最大熵强化学习"></a>C.最大熵强化学习</h3><p>如前所述，在学习过程中寻求最优的方式去探索新动作的时间复杂度很高。最大熵强化学习理论提供了一个有原则的方式去解决这个特定的挑战，并且在很多最近的强化学习方面的高级研究上这已经成为一个关键的部分。例如在一个标准的MDP问题上，最大熵强化学习的目标是需求最优策略$\pi*$ ，这个策略提供了最高的奖励总和期望，然而，它又以最大化每个观测状态的熵。</p><p>$$\pi*=\arg\underset{\pi}{max}\mathbb{E}<em>\pi\{\sum_t^Hr</em>{t+1}+\alpha H(\pi)|s_0=s\}$$</p><p>这里，$\alpha$是温度参数，控制最优策略的随机性，$H(\pi)$代表了策略的熵。在这背后的直觉是找寻返回最高累计奖励同时提供最多各种各样动作分布的策略，这最终能够推进探索。有趣的是，这个方法允许一个状态的平衡在开发和探索方面。对于具有高奖励的状态，一个低熵的策略是允许的。然而，对于低奖励的状态，高熵的策略是首选的，这导致更大的探索。为了简化，折扣因子$\gamma$在方程中被省略了，因为对于最大熵情况来说它导致了更复杂的表达。但是在无穷时间收敛问题上却是需要的，并且包括在我们最后的算法中。</p><p>最大熵强化学习既有概念上的也有实际上的优点。熵导致策略更广泛的探索，也允许近似最佳行为。例如，智能体更偏爱两个同样具有吸引力的行为（更高的熵）而不是一个单一的，限制探索的动作。实际上，改善的探索和更快的学习已经在很多先前的工作中被报道了。</p><h3 id="D-演员-评论家（AC）-方法"><a href="#D-演员-评论家（AC）-方法" class="headerlink" title="D.演员-评论家（AC） 方法"></a>D.演员-评论家（AC） 方法</h3><p>在强化学习文献中，有两种主要智能体训练算法，分别是基于价值的和基于策略的方法。在第一组中，我们有例如Q学习和深度Q网络，这是一种通过首先使用TD算法计算各自的Q函数来估计最优的策略。在DQN算法中，深度神经网络（DNN）在高维度状态空间中被用作Q函数的近似。通过Q函数计算最优策略在离散情况下是简单的，然而在连续动作空间中是不现实的，因为它包含一个额外的最优步骤。在基于策略的方法中，最优策略直接通过定义一个目标函数并且通过调整参数最大化函数值来估计。因此，它们也被称作策略梯度，因为他们包含计算梯度去迭代更新参数。在这种类别中，我们有$REINFORCE$，确定性策略梯度（DPG），信赖域策略优化（TRPO），近端策略优化算法（PPO)和很多其他方法。</p><p>演员评论家方法是一种策略梯度方法，这种方法结合了基于价值和基于策略方法的优点。参数策略模型作为“演员”，它负责选择动作。估计的价值函数作为“评论家”，评判演员坐车的动作。在标准的策略梯度方法中，比如$REINFORCE$，模型参数的更新是通过对带有从整个回合中获得的实际累计奖励$G_t$策略分布增加权重(==推出==)，$\theta\leftarrow\theta+\eta\gamma^tG_t\ln{\pi_\theta (a_t|s_t)}$，这里$\theta$代表模型参数。在演员-评论家算法中，额外的$G_t$被$Q(s,a)，V(s)$，或者是一个优势函数$A(s,a)=Q(s,a)-V(s)$依据特定的变体代替。重要的是，这种改变允许迭代和高效地更新策略和价值函数参数。深度确定性策略梯度（DDPG）是最受欢迎和成功的AC算法之一。DDPG是一种无模型的离线策略方法（使用与当前策略无关的样本更新参数），其中两个神经网络（策略和价值函数）被估计。像DQN方法一样，DDPG使用一个重放缓冲器来稳定参数更新，但是DDPG学习连续动作空间的策略，它与下部分陈述的软演员-评论家（SAC）方法紧密相关。</p><h3 id="E-软演员-评论家"><a href="#E-软演员-评论家" class="headerlink" title="E. 软演员-评论家"></a>E. 软演员-评论家</h3><p>SAC是最成功的基于最大上的方法，并且自从它产生，它就在大部分强化学习书籍中成了一个普遍的基准算法，在很多环境下表现得比DDPG和PPO等最先进的方法还要好。与DDPG方法类似，SAC是一个无模型、离线方法，策略和价值函数通过神经网络近似，但是SAC另外将策略熵项合并到目标函数中，这激励了探索，与软Q学习类似（soft Q-learning）。与DDPG相比，SAC是一种高效且稳定的方法。</p><p>如等式3中表达，SAC策略/演员训练的目标是最大化在特定状态下的期望累计奖励和动作熵。评论家是软Q函数，贝尔曼方程表达形式如下：</p><p>$$Q(s_t,a_t)=r_t+\gamma\mathbb{E}<em>{\rho\pi(s)}[V(s</em>{t+1})]$$,</p><p>其中，$\rho_\pi(s)$代表由策略引发的状态边缘，$\pi(a|s)$，和软价值函数被Q函数参数化：</p><p>$$V(s_{t+1})=\mathbb{E}<em>{a_t\sim\pi}[Q(s</em>{t+1},a_{t+1})-\alpha\log\pi(a_{t+1}|s_{t+1})]$$</p><p>训练软Q函数去最小化下列给定的预测与观测状态-动作价值之间的均方差目标函数：</p><p>$$J_Q=\mathbb{E}<em>{(s_t,a_t)\sim D}[\frac{1}{2}(Q(s_t,a_t)-(r_t+\gamma\mathbb{E</em>{\rho\pi(s)}}[\bar{V}(s_{t+1})]))]$$</p><p>其中，$(s_t,a_t)\sim D$表示从回放缓冲器中采样的状态-动作二元组，$\bar{V}$是目标价值函数。</p><p>最后，通过最小化策略和KL散度指数状态-动作价值函数之间的散度来更新策略（这保证了收敛），并且它可以表示为：</p><p>$$J_\pi=\mathbb{E}<em>{s_t\sim D}[\mathbb{E}</em>{a_t\sim\pi}[\alpha\log\pi(a_t|s_t)-Q(s_t,a_t)]]$$</p><p>其中的思路是最小化动作和状态-动作分布之间的散度。</p><p>SAC算法对于$\alpha$温度参数的改变十分敏感，其中$\alpha$参数依赖于环境，奖励尺度和训练阶段，正如原始论文中所展示的那样。为了解决这个问题，同样的作者提出了一个重要的改进，这个改进就是自动适应温度参数。这个问题现在被阐述为最大熵RL优化但是满足最小熵约束。因此，使用双目标优化方法，最大化期望累计回报，同时最小化期望熵。在实际中，软Q函数和策略网络如更早描述的那样被更新，温度参数$\alpha$函数由具有以下目标的神经网络近似：</p><p>$$J_\alpha=\mathbb{E}_{a_t\sim\pi_t}[-\alpha\log\pi_t(a_t|s_t)-\alpha H_0]$$</p><p>其中，$H_0$是最小期望熵。更深入的细节可以在标注27中找到。下列步骤总结了SAC算法：</p><hr><p>算法1：软演员-评论家</p><hr><ul><li><p>初始化Q，策略和$\alpha$网络参数；</p></li><li><p>初始化目标Q网络权重；</p></li><li><p>初始化回放缓冲器D；</p></li><li><p>对于每一回合集：</p><ul><li><p>对于每个环境步骤：</p><ul><li>从策略函数$\pi(a_t|s_t)$中采样动作，从环境中得到下一状态$s_{t+1}$和奖励$r_t$，将$(s_t,a_t,r_t,s_{t+1})$放入缓冲器D中；</li></ul><p>结束</p></li><li><p>对于每个梯度步骤：</p><ul><li>从缓冲器D中取一组记忆之并且更新Q网络（等式6），策略（等式7），温度参数$\alpha$（等式8），和目标网络权重（软更新）</li></ul><p>结束</p></li></ul><p>结束</p></li></ul><hr><h3 id="F-分层强化学习"><a href="#F-分层强化学习" class="headerlink" title="F.分层强化学习"></a>F.分层强化学习</h3><p>将一个复杂任务分解成更小的任务是许多方法的核心，从经典的分而治之算法到在行动计划中生成子目标。在强化学习中，状态序列的时间抽象用于将问题视为半马尔科夫决策过程（SMDP）。本质上，思路是定义由原始动作组成的宏观动作（例程），这允许了在不同抽象层对智能体进行建模。这种方法就是分层强化学习，它与人类和动物学习的层次结构类似，并在强化学习中产生了重要的进步，例如选项学习（Option-Learning），通用价值函数，选项评论家（Option-Critic），封建网络（Feudal network），数据高效分层RL（HIRO）等等。使用分层强化学习的主要优点是迁移学习（在新任务中使用先前学习的技能和子任务），可扩展性（将大问题分解成小问题，避免了高维度状态空间中的维数灾难）和泛化（较小子任务的组合能生成新的技能，避免了超级专业化）。</p><p>我们使用策略选择器的方法类似于选项学习算法，他与[47]提出的方法密切相关，其中子策略被分层构造去执行新任务。在[47]中，子策略是在类似环境中预训练但是具有不同任务的原始任务。我们的策略选择器（类似于[47]中的主策略）在给定一组预先训练的专业策略（我们称之为低层策略）的情况下学习优化全局奖励。然而，与之前的工作[47]专注于元学习不同的是，我们的主要目标是通过在低层策略之间动态切换来学习与不同对手进行最优化的狗斗。此外，鉴于环境和任务的复杂度，我们不会在策略选择器和子策略的训练之间进行迭代，即在训练策略选择器时不会更新子策略智能体的参数。</p><center><h2>4、ADT仿真环境</h2></center><p>JHU-APL实验室研发并提供了狗斗情形下的环境，并将其作为OpenAI中gym环境之一。F-16的物理模型使用JSBSim进行仿真，这是一个高保真度的，开源的飞行动力学模型。环境的渲染图如图1所示。每个智能体的观测空间包括关于本机信息（燃料负载、推力、控制面偏转、健康度）、空气动力学（$\alpha、\beta$角）、位置（局部平面坐标、速度和加速度）和姿态（欧拉角、速率和加速度）。智能体还获得对手的位置（局部平面坐标和速度）、姿态（欧拉角和速率）信息以及对手的健康状况。环境的所有状态信息在没有建模传感器噪声的情况下提供。</p><p>每个仿真秒动作输入50次。智能体的动作是连续的并映射到F-16的飞控系统输入上（副翼、升降舵、方向舵和油门）的输入。环境给予的奖励基于智能体相对于其对手的位置，其目标是将对手定位在其攻击区（WEZ）内。WEZ被定义为位于2°孔径的球锥内的点的轨迹，该球锥从飞机的前端延伸出来，距离也是500-3000英尺（图2）。尽管智能体并没有真正对对手射击，但在本文中，我们将到达该几何区域称为“射击”。</p><p>在攻击区中每秒对手的损毁程度由下列式子给出：</p><p>$d_{wez}\left\{\begin{array}{l}0&amp;&amp;r&gt;3000ft\\\frac{3000-r}{2500}&amp;&amp;500ft\leq r\leq3000ft\\0&amp;&amp;r&lt;500ft\end{array}\right.$</p><p>其中，$d_{wez}$在攻击去中遭受的损坏，$r$是双方飞机的间距。</p><p>环境基于智能体的奖励计算为对手的损毁随时间曲线下的面积，</p><p>$r_t\left\{\begin{array}{l}\mathbb{E}<em>{t’\in[0,T]}[d</em>{opp}(t’)]&amp;&amp;d_{self}&lt;1\\0&amp;&amp;其他情况\end{array}\right.$</p><p>其中，$r_t$是在时间步$t$时刻的奖励，$d_{self}$是智能体的损毁值，$d_{opp}$是对手的损毁值（图三），最大的持续时间为$T=300$。</p><p>对战从两架飞机都具有完整健康度（健康度为1.0）开始，并在其中的一架或两架飞机达到健康度为0或仿真持续时间到达300秒时结束。智能体当对手的健康值降为0时获胜。当对手下降到1000英尺的最低高度（硬甲板）以下或对手因遭受射击造成足够的伤害时，就会发生这种情况。如果对局超时，对战将以平局结束。</p><center><h2>5、智能体的架构</h2></center><p>我们的智能体，PHANG-MAN（新一代自适应机动策略的策略层次结构）由两层策略层次结构组成。在底层，有一系列经过训练的策略可以在状态空间的特定区域中表现出色。在高层，一个单一的策略选择在当前环境下激活哪个底层策略。我们的架构如图4所示。</p><table><thead><tr><th align="center"></th><th align="center">状态</th><th align="center"></th></tr></thead><tbody><tr><td align="center"></td><td align="center">策略选择器</td><td align="center"></td></tr><tr><td align="center">到达控制区</td><td align="center">激进射击</td><td align="center">保守射击</td></tr><tr><td align="center">动作（副翼、升降舵、方向舵和油门）</td><td align="center"></td><td align="center"></td></tr></tbody></table><center>图4：PHANG-MAN智能体的高层架构</center><h3 id="A-底层策略"><a href="#A-底层策略" class="headerlink" title="A.底层策略"></a>A.底层策略</h3><p>所有的底层策略采用软演员-评论家（SAC）算法训练[49]。由于SAC是一种离线的强化学习算法，演员目的是最大化期望奖励，同时最大化熵，因此探索与表现一同被最大化。每个底层策略都获得相同的状态价值，并具有相同的三层多层感知器（MLP）架构。评估底层策略，并以最大模拟频率50Hz将新动作输入到环境中。底层策略的区别在于训练中使用的初始条件范围和其各自的奖励函数。每个底层策略的奖励函数是各种独立部分的总和，每个部分都旨在鼓励特定的行为。下面给出了奖励函数部分的描述：</p><ul><li>$R_{relative\ position}$当智能体机头指向对手并处于对手后方时给予的奖励。反之惩罚智能体。</li><li>$R_{track\ \theta}$当航迹角（处于机身纵轴和对手中心之间的角度）非零时惩罚智能体。</li><li>$R_{closure}$当追击时，靠近对手给予奖励。被追击时，靠近将收到惩罚。</li><li>$R_{gunsnap(blue)}$是当智能体处于最小航迹角并且在一个特定的距离范围之内所给予的奖励，与环境中攻击区损毁值相似。</li><li>$R_{gunsnap(red)}$是当对手处于最小航迹角并且在一个特定的距离范围之内所给予的惩罚，与环境中攻击区摧毁值相似。</li><li>$R_{deck}$当智能体飞行高度低于最小高度门槛时惩罚。</li><li>$R_{too\ close}$惩罚智能体在一系列不利的角度（对手的尾巴和本机中心之间的角度）内违反了最小距离阈值，旨在阻止追击时过冲。</li></ul><p>三个底层策略的概述分别如下：</p><p>控制区（CZ）：CZ策略将试图在对手身后获得追击位置，并占据状态空间的一个区域，使对手几乎无法逃脱，这也称为“控制区”。</p><p>CZ策略是使用任何底层策略中最广泛的初始条件进行训练的。初始条件由均匀随机位置，欧拉角和机体速度组成。来自退役战斗机飞行员的领域知识被用来射击CZ策略的多维奖励函数。函数独立于航迹角，==逆角==，与对手的距离，硬甲板上方的高度和==关闭率==。给出CZ奖励函数$R_{total}=R_{relative\ position}+R_{closure}+R_{gunsnap(blue)}+R_{gunsnap(red)}+R_{deck}+R_{too\ close}$。CZ策略的奖励函数选择曲面如图5所示。</p><p>激进射击（AS）：与CZ策略不同，AS策略鼓励从侧面进行激进射击并迎头而上。在更近的距离（如攻击区损毁函数），射击的奖励幅度更大。因此，AS策略通常会采取产生最大伤害但容易遭到反击的射击。在防守方面，AS策略避免近距离射击多于远距离的射击，使其成为相对不那么激进的躲避者。AS策略在初始条件下进行训练，将对手或本机置于攻击区内，最大限度地利用学习有效的进攻和防御火炮快速机动的时间。AS奖励函数定义为$R_{total}=R_{track\ \theta}+R_{gunsnap(blue)}+R_{gunsnap(red)}+R_{deck}$。AS策略奖励函数选择曲面如图6（顶部）所示。</p><p>保守射击（CS）：CS策略在训练期间利用了相同的初始条件，并且具有与AS策略相似的奖励函数。显著的区别是CS奖励函数中设计部分的高原区域。结果，CS策略学会了平等地评估近处与远处的射击，从而产生有效保持进攻得分位置的行为，即使得分的幅度可能很低。在防守端，CS策略避免了来自所有距离的射击，使其对所有伤害同样敏感并且相对激进的回避。CS奖励函数定义为：$R_{total}=R_{track \ \theta}+R_{gunsnap(blue)}+R_{gunsnap(red)}+R_{deck}$。CS策略的奖励函数的$R_{gunsnap(blue)}$部分如图6（底部）所示。</p><h3 id="B-策略选择器"><a href="#B-策略选择器" class="headerlink" title="B.策略选择器"></a>B.策略选择器</h3><p>在分层结构的顶层，策略选择网络能够辨别出当前情形下最好的底层策略。我们的方法与选项学习[41]（Options Learning）相似,然而我们没有直接估计终止条件。取而代之的是，以10Hz的频率定期地做出一个新的选择。与选项-评论家[50]（Option-Critic）架构不同的是，可选的策略被分别单独训练并且参数是固定的。这使得策略选择器训练没有与非平稳策略相关的复杂性[45]。这简化了学习问题并允许以模块化的方式训练和重用智能体。策略选择器使用相同的SAC算法实现。尽管离散动作的SAC实现对于选择底层策略[51]是可行的，但SAC连续版本结合最大化参数表现出了更好的性能。</p><p>用于训练策略选择器的奖励函数非常类似攻击区损伤函数，它是稀疏的。为了减轻学习的困难，我们还包含了与每一步的航迹角成正比的奖励。</p><p>在图7中，我们展示了不同的智能体如何根据当前状态动态选择策略。我们绘制了航迹角作为在这个回合集中正在发生什么。在对抗CS策略的比赛中（顶部），策略选择器智能体利用初始位置优势（较小的航迹角），缩近距离的同时减少航迹角，它过冲一次（500步）但得到了持续射击（1100到1300步）并以胜利结束这一回合集。</p><p>在策略选择器训练过程中，每个回合集都会计算底层策略的利用率。当绘制训练集时，可以观察到选择器为每个对手学习独特而有效的策略。图片8展示了在与智能体（如自身（顶部））和随机移动的智能体Randy（底部）对战时，所选的底层策略的标准化利用率如何显著不同。</p><p>我们观察到，策略选择器针对任何单个对手的性能至少与针对同一对手的最高性能底层策略的性能一样好。此外，在多个对手中，策略选择器的性能明显优于任何单个底层策略，表明它能够有效地利用底层策略的优势并产生出独特的互补策略。</p><p>表1给出了底层策略和策略选择器的超参数。</p><center><h2>6、PHANG-MAN在ADT的决赛</h2></center><p>ADT决赛在为期3天的比赛中完成。第1天，参赛者面对JHU-APL与DARPA合作开发的智能体。第2天是循环赛，每支队伍在20场比赛中与其他队伍对抗。第3天，第2天结束时的前4支队伍参加了单场淘汰锦标赛。这些队伍还有机会与美国空军F-16武器教官课程的毕业生进行五局三胜的比赛。我们的智能体在前两天的比赛中以第二名的成绩结束了比三，获得了参加冠军赛的资格。第3天，PHANG-MAN赢得了半决赛，并在决赛中被Heron Systems的智能体击败，最终以第二名的成绩完赛。比赛分组和结果如图9所示。半决赛和决赛可以在线查看。</p><p>在冠军赛中，我们遇到了来自Heron Systems的极具侵略性、高度准确且令人印象深刻的对手。在大多数情况下，PHANG-MAN无法在最初的交火中幸存下来，比赛将以Heron获胜而结束，但是剩余的生命值很少。尽管我们对Heron的之恩那个提的总射击分数增加了7%，但我们的平均射击距离更远，因此我们的平均伤害更低。我们的智能体将在800英尺内停止进攻，以便为下一次交火提供更好的定位，而Heron的智能体将继续积极追击。由于这种侵略性，当在最初的交火中幸存下来时，PHANG-MAN能够获得一个进攻性的指挥位置。</p><p>我们怀疑这种对未来定位而不是即时得分的偏见是我们在训练期间使用的策略的结果，在该策略中，我们人为地将智能体的健康度提高了10倍。此外，由于没有提供与对手剩余生命值成比例的奖励或完全耗尽对手生命值的奖励，我们可能无意中让我们的智能体学会放弃接近胜利。</p><p>总体而言，PHANG-MAN展示了对激进和保守战术的战略使用。我们相信它有潜力成为值得信赖的副驾驶或僚机，可以有效地执行现实世界中自我保护至关重要的战术。</p><p>对于顶级竞争对手和人类空军飞行员之间的比赛，DARPA和JHU-APL团队提供了一个高保真VR驾驶舱（图10）。这使得人类飞行员能够像在通常的实际交战中那样在视觉上跟踪AI对手。为飞行员显示了一个仪表板，提供了相关信息的简化视图（例如航迹角、与对手的相对距离、高度、燃料等）。作为额外的视觉辅助，当飞行员在他的视野之外时，会提供一个向飞行员指出对手方向的图标，以及当他遭受射击时整个飞行员视野的红色闪烁。</p><p>PHANG-MAN是在与美国空军武器教官飞行员的五场比赛中获胜（5W-0L）的四个智能体之一。比赛特点是PHANG-MAN从正面和侧面进行激进的射击，同时还利用人类飞行员犯下的任何错误而放弃了他们的控制区。</p><center><h2>7、未来的工作</h2></center><p>ADT一年的快节奏时间表为该问题领域的未来研究提供了许多新途径。为此，洛克希德马丁公司正在投入大量资金来进一步研究和开发这项工作。</p><p>除了针对空对空领域的更深入的算法研究外，我们正在调整我们目前的工作，以实现全面部署的目标。增强模拟环境是朝着这个方向迈出的第一步：修改和增加观察空间（结合领域随机化技术[52]、不完全信息、部分/估计知识等）并结合不同平台的空气动力学模型（四轴飞行器、小型固定翼无人机、F-22等）。当我们部署在小规模和最终全尺寸飞机上时，过渡将更加平稳。</p><center><h2>8、结束</h2></center><p>Alpha狗斗实验旨在挑战竞争对手，以开发能够在空战中表现出色的高性能AI智能体。我们研发的分层结构智能体在竞争中取得成功，总体排名第二，并击败了美国空军武器学校F-16武器教官课程的毕业生。LM打算继续投资于这项研究，并充分探索这些算法的潜力及其部署路径。</p><center><h2>鸣谢</h2></center><p>感谢 DARPA 和 JHU-APL 主持该项目。 感谢为该项目做出重大贡献的研发团队成员：Matt Tarascio（洛克希德马丁公司人工智能副总裁）谢万林、John Stanco、Brandon Liston 和 Jason “Vandal” Garrison。</p><p>本文基于国防高级研究计划局 (DARPA) 赞助的工作。 所表达的任何意见、调查结果、结论或建议均为作者的观点，并不一定反映DARPA、美国空军、美国国防部或美国政府的观点。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/hello-world/"/>
      <url>/hello-world/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
